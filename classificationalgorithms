#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 16 11:53:15 2019

@author: vivek
"""

import pandas as pd
import matplotlib.pyplot as plt
fruits = pd.read_table('/Users/vivek/Desktop/git/Ai/classificationalgorithms/fruit_data.txt')
print(fruits.head())

print(fruits.shape)#59 observation with 7 variable (predictor as well as target variables)
print(fruits['fruit_name'].unique())
print(fruits.groupby('fruit_name').size())

import seaborn as sns
sns.countplot(fruits['fruit_name'], label ="count")
plt.show()

fruits.drop('fruit_label',axis=1).plot(kind='box',subplots=True,layout=(2,2),sharex=False,sharey=False,figsize=(9,9),title='Box plot for each input variable')
#plt.savefig('fruit_box')
plt.show()

import pylab as pl
fruits.drop('fruit_label',axis=1).hist(bins=30,figsize=(9,9)) # frequency of occurence - histogram
pl.suptitle("histogram for each numeric input variables")
plt.show()
#co-relation can be underestood from these 


features_names=['mass','width','height','color_score']
X = fruits[features_names]
Y = fruits['fruit_label']


from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=0)


#variables are in high as well as low range to make it similar in the ranges we use this
from sklearn.preprocessing import MinMaxScaler
scaler =MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#LOGISTIC REGRESSION ALGORITHM

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train,Y_train)
print('Accuracy of the Logistic Regression classifier on training set: {:.2f}'.format(logreg.score(X_train,Y_train)))
print('Accuracy of the Logistic Regression on the test case: {:.2f}'.format(logreg.score(X_test,Y_test)))

#Decision tree
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier().fit(X_train,Y_train)
print('Accuracy of Descision Tree classifier on the training set: {:.2f}'.format(clf.score(X_train,Y_train)))
print('Accuracy of Descision Tree classifier on the test set: {:.2f}'.format(clf.score(X_test,Y_test)))

#KNN classifier
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(X_train,Y_train)
print('Accuracy of K-NN classifier on the training set: {:.2f}'.format(knn.score(X_train,Y_train)))
print('Accuracy od the K-NN classifier on the test set: {:.2f}' .format(knn.score(X_test,Y_test)))


#NaiveBayes
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train,Y_train)
print('Accuracy of GNB classifier on training set: {:.2f}' .format(gnb.score(X_train,Y_train)))
print ('Accuracy of GNB classifier on the test set: {:.2f}' .format(gnb.score(X_test,Y_test)))


#Support Vector Machines
from sklearn.svm import SVC
svm=SVC()
svm.fit(X_train,Y_train)
print('Accuracy of SVM classifier on the training set: {:.2f}' .format(svm.score(X_train,Y_train)))
print('Accuracy of SVM classifier on the training set: {:.2f}' .format(svm.score(X_train,Y_train)))

gamma = 'auto'
#Confusion Matrix for knn classifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
pred = knn.predict(X_test)
print(confusion_matrix(Y_test,pred))
print(classification_report(Y_test,pred))












